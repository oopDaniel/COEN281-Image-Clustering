{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_pre\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_pre\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering, AgglomerativeClustering, OPTICS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import normalized_mutual_info_score, silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DIR = \"data/traffic-small\"\n",
    "FINAL_DIR = \"data/traffic\"\n",
    "dataset_dir = FINAL_DIR\n",
    "is_sampling = dataset_dir == SAMPLE_DIR\n",
    "\n",
    "# Sort is the key :/\n",
    "dataset = sorted(glob.glob(dataset_dir + \"/*.jpg\"))\n",
    "\n",
    "# Where we store the predictions\n",
    "RESULT_DIR = \"res\"\n",
    "# Where we store the extracted features\n",
    "FEATURE_SAVING_DIR = \"extracted_features\"\n",
    "\n",
    "pretrain_input_size = (54, 28)\n",
    "\n",
    "N_CLUSTERS = 14\n",
    "\n",
    "CPUS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_lines(path):\n",
    "    r\"\"\" Open text file by path and read all lines \"\"\"\n",
    "    with open(path, \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        \n",
    "    # transform docs into lists of words\n",
    "    raw_lines = [l.split() for l in lines]\n",
    "\n",
    "    return raw_lines\n",
    "\n",
    "def save_result_as_file(prediction, file_name=\"prediction.dat\"):\n",
    "    r\"\"\" Save the predicted result as a new file \"\"\"\n",
    "    file_content = \"\\n\".join(list(map(str, prediction)))\n",
    "    path = os.path.join(RESULT_DIR, file_name)\n",
    "    with open(path, \"w\") as fd:\n",
    "        fd.write(file_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_formatted_name(prediction, model, net=\"res50\", pca=False, random_id=None):\n",
    "    fname = \"%s_%s_%s%s%s.dat\" % (\n",
    "        model.__class__.__name__,\n",
    "        net,\n",
    "        \"%sx%s\" % pretrain_input_size,\n",
    "        \"_pca\" if pca else \"\",\n",
    "        \"_rand%s\" % random_id if random_id else \"\"\n",
    "    )\n",
    "    save_result_as_file(prediction, fname)\n",
    "    print(\"Saved %s\" % fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x):\n",
    "    np.take(x, np.random.permutation(x.shape[0]), axis=0, out=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics\n",
    "Analyze the data distribution first, so we can decide how to resize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(dataset):\n",
    "    dim_to_key = lambda dim: \"({},{})={}\".format(dim[0], dim[1], dim[0] * dim[1])\n",
    "    \n",
    "    dim_cnt = Counter()\n",
    "    multiply_dim_cnt = Counter()\n",
    "\n",
    "    images = []\n",
    "    for path in dataset:\n",
    "        img = cv2.imread(path)\n",
    "        key = dim_to_key(img.shape)\n",
    "        dim_cnt[key] += 1\n",
    "        multiply_dim_cnt[img.shape[0] * img.shape[1]] += 1\n",
    "        \n",
    "    print(dim_cnt.most_common())\n",
    "    print(multiply_dim_cnt.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(dataset):\n",
    "    images = []\n",
    "    for path in dataset:\n",
    "        img = image.load_img(path, target_size=pretrain_input_size)\n",
    "        img = image.img_to_array(img)\n",
    "    #     img = cv2.imread(path)\n",
    "    #     img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    #     img = cv2.resize(img, pretrain_input_size)\n",
    "    #     img = img / 255.0  # Normalization\n",
    "        images.append(img)\n",
    "        \n",
    "    images = np.array(images)\n",
    "    print(images.shape)\n",
    "    \n",
    "    return images\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_feature_saving_dir():\n",
    "    if not os.path.exists(FEATURE_SAVING_DIR) or \\\n",
    "        len(os.listdir(FEATURE_SAVING_DIR)) == 0:\n",
    "        try:\n",
    "            shutil.rmtree(FEATURE_SAVING_DIR)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        os.mkdir(FEATURE_SAVING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_MODELS = [\"vgg\", \"resnet50\"]\n",
    "\n",
    "def save_batch_features(x,\n",
    "                        model_name=\"vgg\",\n",
    "                        preprocessing=False,\n",
    "                        pooling=None,\n",
    "                        save_as_file=True,\n",
    "                        filename=None):\n",
    "\n",
    "    assert model_name in SUPPORTED_MODELS\n",
    "    \n",
    "    init_feature_saving_dir()\n",
    "    input_dim = pretrain_input_size + (3,) # (25,25) -> (25,25,3)\n",
    "    \n",
    "    if model_name == \"vgg\":\n",
    "        if preprocessing:\n",
    "            x = vgg16_pre(x)\n",
    "        model = VGG16(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      pooling=pooling)\n",
    "        \n",
    "    elif model_name == \"resnet50\":\n",
    "        if preprocessing:\n",
    "            x = resnet50_pre(x)\n",
    "        model = ResNet50(weights='imagenet',\n",
    "                         include_top=False,\n",
    "                         pooling=pooling)\n",
    "\n",
    "    y = model.predict(x,\n",
    "                      workers=CPUS,\n",
    "                      use_multiprocessing=True)\n",
    "    \n",
    "    print(y.shape)\n",
    "\n",
    "    if save_as_file or filename:\n",
    "        name = filename if filename else \\\n",
    "            \"features-%s_%s%s%s%s\" % (\n",
    "                model_name,\n",
    "                \"%sx%s\" % pretrain_input_size,\n",
    "                \"_%s\" % pooling if pooling else \"\",\n",
    "                \"_%s\" % \"preprocess\" if preprocessing else \"\",\n",
    "                \"_sample\" if is_sampling else \"_final\",\n",
    "            )\n",
    "        np.save(\"%s/%s\" % (\n",
    "            FEATURE_SAVING_DIR, \n",
    "            name\n",
    "        ), y)\n",
    "        print(\"Saved as %s\", name)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    return np.array([np.ndarray.flatten(img) for img in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_data(x_train, do_the_math=True, reduced_n_components=2400):\n",
    "    def percvar(v):\n",
    "        r\"\"\"Transform eigen/singular values into percents.\n",
    "        Return: vector of percents, prefix vector of percents\n",
    "        \"\"\"\n",
    "        # sort values\n",
    "        s = np.sort(np.abs(v))\n",
    "        # reverse sorting order\n",
    "        s = s[::-1]\n",
    "        # normalize\n",
    "        s = s/np.sum(s)\n",
    "        return s, np.cumsum(s)\n",
    "\n",
    "    def perck(s, p):\n",
    "        return next(i + 1 for i, v in enumerate(s) if v >= p)\n",
    "\n",
    "    if do_the_math:\n",
    "        X_std = StandardScaler().fit_transform(x_train)\n",
    "        means = np.mean(X_std, axis=0)\n",
    "        X_sm = X_std - means\n",
    "\n",
    "        U,s,V = np.linalg.svd(X_sm)\n",
    "        _, pv = percvar(s**2/(X_sm.shape[0]-1))\n",
    "\n",
    "        percentage_explained = 99\n",
    "        n_components = perck(pv, percentage_explained * 0.01)\n",
    "    else:\n",
    "        n_components = reduced_n_components\n",
    "    \n",
    "    print(\"Original: %d. After PCA: %d\" % (x_train.shape[-1], n_components))\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    return svd.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_test_file(x, pca=False):\n",
    "    r\"\"\"Let's not waste submissions and hope it's not too bad\"\"\"\n",
    "    shuffle(x)\n",
    "    random_state = np.random.randint(0, len(x) - 1)\n",
    "    model = KMeans(n_clusters=N_CLUSTERS,\n",
    "                   n_jobs=CPUS,\n",
    "                   random_state=random_state)\n",
    "    model.fit(x)\n",
    "    prediction = model.labels_.tolist()\n",
    "    save_formatted_name(prediction, model, pca=pca, random_id=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(x, pca=False):\n",
    "    # Generate random K-means to submit today\n",
    "#     generate_random_test_file(x, pca=True)\n",
    "\n",
    "#     model = AgglomerativeClustering(n_clusters=N_CLUSTERS, memory=\"tmp\",\n",
    "#                                  affinity=\"cosine\", linkage=\"complete\")\n",
    "#     model = AgglomerativeClustering(n_clusters=N_CLUSTERS, memory=\"tmp\",\n",
    "#                                  affinity=\"cosine\", linkage=\"average\")\n",
    "#     model = AgglomerativeClustering(n_clusters=N_CLUSTERS, memory=\"tmp\",\n",
    "#                                  affinity=\"euclidean\", linkage=\"ward\")\n",
    "#     model = DBSCAN(eps=49.8, min_samples=3, n_jobs=CPUS)\n",
    "#     model = KMeans(n_clusters=N_CLUSTERS, n_jobs=CPUS,\n",
    "#                    random_state=np.random.randint(0, 100000))\n",
    "#     model = OPTICS(min_samples=20, n_jobs=CPUS)\n",
    "    model = SpectralClustering(n_clusters=N_CLUSTERS, random_state=4,\n",
    "                           n_init=50, n_neighbors=10,\n",
    "                          affinity=\"nearest_neighbors\", n_jobs=CPUS)\n",
    "    model.fit(x)\n",
    "    prediction = model.labels_\n",
    "    print(np.unique(prediction, return_counts=True))\n",
    "    \n",
    "    if is_sampling:\n",
    "        y = load_data_as_lines(os.path.join(dataset_dir, \"clusters.txt\"))\n",
    "        y = np.array(y).flatten().astype(int)\n",
    "        nmi_score = normalized_mutual_info_score(y, prediction)\n",
    "        print(\"NMI score: \", nmi_score)\n",
    "        \n",
    "    s_score = silhouette_score(x, prediction)\n",
    "    print(\"Silhouette score: \", s_score)\n",
    "\n",
    "    save_formatted_name(prediction, model, pca=pca)\n",
    "    \n",
    "    \n",
    "    ##### Dim (54, 28)\n",
    "    # RESNET50 + AgglomerativeClustering(elu, ward) 0.3769453717166426\n",
    "    # RESNET50_PCA + AgglomerativeClustering(elu, ward) 0.3844810789874803\n",
    "    # RESNET50_PCA + AgglomerativeClustering(cosine, complete) 0.36958888818960556\n",
    "\n",
    "    # RESNET50 + KMEANS 0.36985722497762685\n",
    "    # RESNET50_PCA + KMEANS 0.28\n",
    "\n",
    "    # RESNET50 + DBSCAN(49.8, 3) 0.02377917875872419\n",
    "\n",
    "    ##### Bad dimensions (64, 64)\n",
    "    # VGG16_MAX + KMEANS 0.008307981394281364\n",
    "    # VGG16_MAX_PCA + KMEANS 0.009747882300356864\n",
    "    # VGG16 + KMEANS 0.009683774163696106\n",
    "    # VGG16_PCA + KMEANS 0.010623286038995799\n",
    "    # RESNET50 + KMEANS 0.009794626073636827\n",
    "    # RESNET50_PCA + KMEANS 0.008068110910338717\n",
    "    # RESNET50_MAX + KMEANS 0.01032285854898277\n",
    "    # RESNET50_MAX_PCA + KMEANS 0.008569389462865706\n",
    "\n",
    "    # VGG16 + DBSCAN(0.5, 5) 0.024794674645757197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_cure(x):\n",
    "    r\"\"\"This doesn't work well with our data. Just keep it here as a ref.\"\"\"\n",
    "    cure_instance2 = cure(x, N_CLUSTERS,\n",
    "                      number_represent_points=40,\n",
    "                      compression=0.05)\n",
    "\n",
    "    cure_instance2.process()\n",
    "    \n",
    "    clusters = cure_instance2.get_clusters()\n",
    "    \n",
    "    sample_size = 4209\n",
    "    pred = [False for _ in range(sample_size)]\n",
    "\n",
    "    label = 0\n",
    "    for cluster in clusters:\n",
    "        label += 1\n",
    "        for i in cluster:\n",
    "            pred[i] = label\n",
    "            \n",
    "    print(list(filter(lambda x: x > 1, pred))) # Count of non-dominating clusters\n",
    "            \n",
    "    y = load_data_as_lines(os.path.join(dataset_dir, \"clusters.txt\"))\n",
    "    y = np.array(y).flatten().astype(int)\n",
    "    nmi_score = normalized_mutual_info_score(y, prediction)\n",
    "    print(\"NMI score: \", nmi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Optional\n",
    "#     analyze_data(dataset)\n",
    "\n",
    "    images = preprocess_img(dataset)\n",
    "    \n",
    "    features = save_batch_features(images, \"resnet50\")\n",
    "    \n",
    "    # Load extracted features locally\n",
    "    x = features\n",
    "    # Load final for processing\n",
    "#     x = np.load(os.path.join(FEATURE_SAVING_DIR, \"features-resnet50_54x28_final_7.npy\"))\n",
    "    # Load best random sample so far to evaluate metrics\n",
    "#     x = np.load(os.path.join(FEATURE_SAVING_DIR, \"resnet50_54x28_pca_s.npy\"))\n",
    "    \n",
    "    x = flatten(x)\n",
    "    \n",
    "    # Optional PCA\n",
    "#     x = get_pca_data(x)\n",
    "#     np.save(\"%s/%s\" % (FEATURE_SAVING_DIR, \"resnet50_%sx%s_pca\" % pretrain_input_size), x)\n",
    "#     x = np.load(os.path.join(FEATURE_SAVING_DIR, \"resnet50_54x28_pca.npy\"))\n",
    "\n",
    "# shuffle(x)\n",
    "    \n",
    "    clustering(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
